{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Media Mining: Sentiment Analysis\n",
    "### Vincent Malic - Spring 2018\n",
    "## Module 7.1 Sentiment Analysis\n",
    "* Sentiment Analysis is a *huge* subfield of text mining. \n",
    "* Identifying, extracting subjective information from source material text. \n",
    "* Overarching goal is the identification of subjectivity. \n",
    "\n",
    "## Canonical reference text\n",
    "* Opinion Mining and Sentiment Analysis (2009) Bo Pang and Lillian Lee \n",
    "(http://www.cs.cornell.edu/home/llee/omsa/omsa.pdf). \n",
    "* Treatise on *problem* of sentiment analysis, relevant today. \n",
    "* Much of talk and demos will be based, directly or indirectly, on Pang's article. \n",
    "\n",
    "\n",
    "# Implementing basic sentiment analysis approaches: \n",
    "\n",
    "## Polarity Classification\n",
    "* Sentiment analysis in its most simple form: Is a sentence positive or negative? \n",
    "* Binary classification: Assign value of 1 if positive, or 0 if negative. \n",
    "* Two mutually exclusively and oppositional *polarities* and you're assuming that a given text belongs to one or the other. \n",
    "\n",
    "### How do we train a computer to do this? \n",
    "* Identify patterns in text and classify new, unseen, incoming texts as positive or negative?\n",
    "* Classify texts based on positive/negative adjectives (clear polarity): \"terrific\", \"sucks\" \n",
    "\n",
    "## Example: Analyze most recent 2500 Tweets from Clinton and Trump \n",
    "* Use Tweepy API to pull Tweets, downloaded and saved as csv file using Pandas. \n",
    "* Load data into Python environment. The csv file will be available on canvas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"sentimentanalysis.csv\", index_col=0, encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>fav</th>\n",
       "      <th>rt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>Remember, don't believe \"sources said\" by the ...</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>8572</td>\n",
       "      <td>3616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>Did Crooked Hillary help disgusting (check out...</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>12930</td>\n",
       "      <td>5628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>Using Alicia M in the debate as a paragon of v...</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>12738</td>\n",
       "      <td>5209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>Wow, Crooked Hillary was duped and used by my ...</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>13210</td>\n",
       "      <td>5205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>Anytime you see a story about me or my campaig...</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>21316</td>\n",
       "      <td>9147</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              user                                               text  \\\n",
       "0  realDonaldTrump  Remember, don't believe \"sources said\" by the ...   \n",
       "1  realDonaldTrump  Did Crooked Hillary help disgusting (check out...   \n",
       "2  realDonaldTrump  Using Alicia M in the debate as a paragon of v...   \n",
       "3  realDonaldTrump  Wow, Crooked Hillary was duped and used by my ...   \n",
       "4  realDonaldTrump  Anytime you see a story about me or my campaig...   \n",
       "\n",
       "                source    fav    rt  \n",
       "0  Twitter for Android   8572  3616  \n",
       "1  Twitter for Android  12930  5628  \n",
       "2  Twitter for Android  12738  5209  \n",
       "3  Twitter for Android  13210  5205  \n",
       "4  Twitter for Android  21316  9147  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Brute-force sentiment analysis - simple classification: \"good\"/\"bad\"\n",
    "* Iterate through all the tweet texts and mark each tweet that contains the word \"good\" as 1 and each tweet that contains the word \"bad\" as -1. \n",
    "* Leave neutral category, 0, for likely possibility that a tweet does not contain either word. This will be our first, \n",
    "* Note that in its current format, the Tweet text is a string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Remember, don\\'t believe \"sources said\" by the VERY dishonest media. If they don\\'t name the sources, the sources don\\'t exist.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "* Method for splitting each tweet into a list of words  \n",
    "* Use Natural Language Toolkit,``nltk``, word tokenizer. \n",
    "* First, intsall nltk at command line and install some peripherals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'sentence', 'is', 'a', 'string', ',', 'but', 'after', 'it', \"'s\", 'passed', 'through', 'word_tokenize', ',', 'it', 'becomes', 'a', 'list', 'of', 'words', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "print(word_tokenize(\"This sentence is a string, but after it's passed through word_tokenize, it becomes a list of words.\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Iterate through all the tweets:\n",
    "* Tokenize it, and see if a positive or negative word is in it, and assign it a label.\n",
    "* Add newly-created sentiment labels to data frame as new column\n",
    "* if the tweet contains 'good' append 1, if tweet contains 'bad' append -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize empty list to store values\n",
    "sentiment_labels = []\n",
    "\n",
    "for text in df['text']:\n",
    "    tokenized_text = word_tokenize(text)\n",
    "    if \"good\" in tokenized_text:\n",
    "        sentiment_labels.append(1) # If positive tweet; append 1\n",
    "    elif \"bad\" in tokenized_text:\n",
    "        sentiment_labels.append(-1) # If negative tweet; append -1\n",
    "    else:\n",
    "        sentiment_labels.append(0) # If no identifiable sentiment (by simple model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>fav</th>\n",
       "      <th>rt</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>Remember, don't believe \"sources said\" by the ...</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>8572</td>\n",
       "      <td>3616</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>Did Crooked Hillary help disgusting (check out...</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>12930</td>\n",
       "      <td>5628</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>Using Alicia M in the debate as a paragon of v...</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>12738</td>\n",
       "      <td>5209</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>Wow, Crooked Hillary was duped and used by my ...</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>13210</td>\n",
       "      <td>5205</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>Anytime you see a story about me or my campaig...</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>21316</td>\n",
       "      <td>9147</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              user                                               text  \\\n",
       "0  realDonaldTrump  Remember, don't believe \"sources said\" by the ...   \n",
       "1  realDonaldTrump  Did Crooked Hillary help disgusting (check out...   \n",
       "2  realDonaldTrump  Using Alicia M in the debate as a paragon of v...   \n",
       "3  realDonaldTrump  Wow, Crooked Hillary was duped and used by my ...   \n",
       "4  realDonaldTrump  Anytime you see a story about me or my campaig...   \n",
       "\n",
       "                source    fav    rt  sentiment  \n",
       "0  Twitter for Android   8572  3616          0  \n",
       "1  Twitter for Android  12930  5628          0  \n",
       "2  Twitter for Android  12738  5209          0  \n",
       "3  Twitter for Android  13210  5205          0  \n",
       "4  Twitter for Android  21316  9147          0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'] = sentiment_labels\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine results. \n",
    "* How many good, bad, and neutral tweets did we find for Clinton and Trump?\n",
    "* Use groupby() function to obtain summary descriptive for different variables\n",
    "* Group by 'user, aggregated count by group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user             sentiment\n",
       "HillaryClinton   -1              7\n",
       "                  0           2446\n",
       "                  1             47\n",
       "realDonaldTrump  -1             64\n",
       "                  0           2393\n",
       "                  1             43\n",
       "Name: user, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby([\"user\", \"sentiment\"])[\"user\"].agg(\"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subset the DataFrame\n",
    "* Look at what positive and negative Tweets we found for the candidates.  \n",
    "* Subset tweets by Hillary Clinton with negative sentiment (value=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clinton, Negative\n",
      "\"Donald Trump doesn't see people like me, he only sees disability. I honestly feel bad for someone with so much hat",
      " https://t.co/NTeF9u8JpN\n",
      "**************************************************\n",
      "RT @RevDrBarber: Not only is @realDonaldTrump wrong on birtherism, his words and policies are bad for Americans alive now and our children",
      "\n",
      "**************************************************\n",
      "3. While refusing to release your tax returns, how will you confirm that you do not have dangerous financial ties to bad actors abroad?\n",
      "**************************************************\n",
      "A wall that Mexico will pay for: a bad idea from an even worse negotiator. https://t.co/RZfOKe45RO\n",
      "**************************************************\n",
      "Donald Trump's economic plan is a bad deal for working familiesand a big tax cut for Donald Trump.\r\n",
      "https://t.co/D0oOe6bwza\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "c_neg = df[(df['user']==\"HillaryClinton\") & (df['sentiment']==-1)]\n",
    "print(\"Clinton, Negative\")\n",
    "\n",
    "for t in c_neg['text'][:5]:\n",
    "    print(t)\n",
    "    print(\"*\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple model correctly classifies negative tweets that are indeed negative. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trump, Negative\n",
      ".@HillaryClinton channels John Kerry on trade: she was for bad trade deals before she was against them. #TPP #Debates2016\n",
      "**************************************************\n",
      "Crooked Hillary's bad judgement forced her to announce that she would go to Charlotte on Saturday to grandstand. Dem pols said no way, dumb!\n",
      "**************************************************\n",
      "President Obama &amp; Putin fail to reach deal on Syria - so what else is new? Obama is not a natural deal maker. Only makes bad deals!\n",
      "**************************************************\n",
      "Now that African-Americans are seeing what a bad job Hillary type policy and management has done to the inner-cities, they want TRUMP!\n",
      "**************************************************\n",
      "Crooked Hillary's brainpower is highly overrated.Probably why her decision making is so bad or, as stated by Bernie S, she has BAD JUDGEMENT\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "t_neg = df[(df['user']==\"realDonaldTrump\") & (df['sentiment']==-1)]\n",
    "print(\"Trump, Negative\")\n",
    "\n",
    "for t in t_neg['text'][:5]:\n",
    "    print(t)\n",
    "    print(\"*\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now see if tweets labeled as positive are indeed positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clinton, Positive\n",
      "When you have a really, really good night. #SheWon https://t.co/FVeGqhYxRZ\n",
      "**************************************************\n",
      "\"Anyone who complains about microphone problems is not having a good night.\" Hillary #SheWon\n",
      "**************************************************\n",
      "\"Maybe he didn't do a good job.\" Donald Trump\r\n",
      "\r\n",
      "Looks like you loved it at the time. #DebateNight",
      " https://t.co/LelC6Tb3nj\n",
      "**************************************************\n",
      "Donald Trump is really good at spending other people's moneyon:\r\n",
      "\r\n",
      "Legal settlements.\r\n",
      "Payoffs.\r\n",
      "Portraits of himself. https://t.co/C5bXw5brI9\n",
      "**************************************************\n",
      "\"You want to give me a good send-off? Go vote. Barack Obama: https://t.co/tTgeqxNqYm https://t.co/Jqf2jmx3D0\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "c_pos = df[(df['user']==\"HillaryClinton\") & (df['sentiment']==1)]\n",
    "print(\"Clinton, Positive\")\n",
    "\n",
    "for t in c_pos['text'][:5]:\n",
    "    print(t)\n",
    "    print(\"*\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Difficulties of sentiment analysis:\n",
    "* Evaluate sentiment of 3rd tweet? contains \"good\" but has *negated* sentiment. \n",
    "* Tweet contains a *quote* which is hard to label as as positive or negative. \n",
    "* Tweet 4 contains sarcasm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trump, Positive\n",
      "Heading to Colorado for a big rally. Massive crowd, great people! Will be there soon - the polls are looking good.\n",
      "**************************************************\n",
      "Poll numbers are starting to look very good. Leading in Florida @CNN Arizona and big jump in Utah. All numbers rising, national way up. Wow!\n",
      "**************************************************\n",
      "\"@Brainykid2010: @shl @realDonaldTrump The ad was actually very good!\"\n",
      "**************************************************\n",
      "You have no idea what my strategy on ISIS is, and neither does ISIS (a good thing). Please get your facts straight - thanks. @megynkelly\n",
      "**************************************************\n",
      "Funny, if you listen to @FoxNews, the Democrats did not have a good day. If you listen to the other two, they are fawning. What a difference\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "t_pos = df[(df['user']==\"realDonaldTrump\") & (df['sentiment']==1)]\n",
    "print(\"Trump, Positive\")\n",
    "\n",
    "for t in t_pos['text'][:5]:\n",
    "    print(t)\n",
    "    print(\"*\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge-Based Sentiment Analysis\n",
    "* Human-made, pre-existing knowledge base to make our sentiment classification decisions.\n",
    "* Two-item knowledge base: \"good\" indicates positive tweet, \"bad\" indicates negative tweet. \n",
    "\n",
    "### More powerful classification scheme needs to account for sublety\n",
    "* Better knowledge base includes *more indicator words*. \n",
    "* Create pool of words that indicate positivity, another pool of words indicating negativity.\n",
    "* Use scoring system rather than working in binary (\"awful\" = -2, \"bad = -1). \n",
    "\n",
    "## Dictionary of positive and negative \"weights\"\n",
    "* Assign a *total score* to each Tweet by summing the weights, of pos. and neg. words \n",
    "* Researchers have constructed lists of positive and negative words *and* assigned weights. \n",
    "\n",
    "## AFINN dataset - Finn Nielsen\n",
    "* http://www2.imm.dtu.dk/pubdb/views/publication_details.php?id=6010\n",
    "* 2477 words and phrases with different weights annotated as such.\n",
    "\n",
    "### Examples of andom words from AFINN:\n",
    "* abandon -2\n",
    "* affection 3\n",
    "* anxious -2\n",
    "* applauded 2\n",
    "* best 3\n",
    "\n",
    "### To obtain a polarity score for a given text,\n",
    "* Identify all words in the text that are in AFINN, and add up their corresponding weight. \n",
    "* If it's above 0, it's mostly positive, if it's below, it's mostly negative. \n",
    "* How much improvement do we gain with this wider knowledge base?\n",
    "\n",
    "## Python module ``afinn``. \n",
    "* Install afinn module from command line or terminal environment: \n",
    "```\n",
    "pip install afinn\n",
    "```\n",
    "\n",
    "### Import factory method to get it installed:\n",
    "* Next make an instance of an ``afinn`` object. \n",
    "* Then, call its method score and pass it a string of text. \n",
    "* NLTK will tokenize the text, using ``afinn``: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from afinn import Afinn\n",
    "\n",
    "afinn = Afinn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "afinn.score(\"What a wonderful day! The sun is shining and the birds are singing. I feel great.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-4.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "afinn.score(\"What a dreary and depressing day. It's raining and I don't even own an umbrella.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an empty list called `pscore` (for \"polarity score\"). \n",
    "* Iterate through all Tweet texts and \n",
    "* Append its corresponding AFINN score to `pscore` list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-2.0, -1.0, -5.0, -6.0, -1.0]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pscore = []\n",
    "\n",
    "for text in df['text']:\n",
    "    pscore.append(afinn.score(text))\n",
    "    \n",
    "pscore[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add `pscores` as a column to our Data Frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>source</th>\n",
       "      <th>fav</th>\n",
       "      <th>rt</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>pscore</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>Remember, don't believe \"sources said\" by the ...</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>8572</td>\n",
       "      <td>3616</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>Did Crooked Hillary help disgusting (check out...</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>12930</td>\n",
       "      <td>5628</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>Using Alicia M in the debate as a paragon of v...</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>12738</td>\n",
       "      <td>5209</td>\n",
       "      <td>0</td>\n",
       "      <td>-5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>Wow, Crooked Hillary was duped and used by my ...</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>13210</td>\n",
       "      <td>5205</td>\n",
       "      <td>0</td>\n",
       "      <td>-6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>realDonaldTrump</td>\n",
       "      <td>Anytime you see a story about me or my campaig...</td>\n",
       "      <td>Twitter for Android</td>\n",
       "      <td>21316</td>\n",
       "      <td>9147</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              user                                               text  \\\n",
       "0  realDonaldTrump  Remember, don't believe \"sources said\" by the ...   \n",
       "1  realDonaldTrump  Did Crooked Hillary help disgusting (check out...   \n",
       "2  realDonaldTrump  Using Alicia M in the debate as a paragon of v...   \n",
       "3  realDonaldTrump  Wow, Crooked Hillary was duped and used by my ...   \n",
       "4  realDonaldTrump  Anytime you see a story about me or my campaig...   \n",
       "\n",
       "                source    fav    rt  sentiment  pscore  \n",
       "0  Twitter for Android   8572  3616          0    -2.0  \n",
       "1  Twitter for Android  12930  5628          0    -1.0  \n",
       "2  Twitter for Android  12738  5209          0    -5.0  \n",
       "3  Twitter for Android  13210  5205          0    -6.0  \n",
       "4  Twitter for Android  21316  9147          0    -1.0  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['pscore'] = pscore\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Investigate how well this method works. \n",
    "* First, split the data frame into two subsets, \n",
    "* One subset containing the tweets for a single candidate.\n",
    "* Now, look at the Clinton and Trump tweets with the highest negative polarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clinton = df[df['user'] == \"HillaryClinton\"]\n",
    "trump = df[df['user'] == \"realDonaldTrump\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sort `pscore values` with `sort_values` method \n",
    "* Takes argument called ``by`` which indicates which column you want to sort by. \n",
    "* Updates the dataFrame, sorting, in this case, from lowest to highest, \n",
    "* The top five rows will be the **most negative** (have the lowest values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRUMP, LOWEST POLARITY\n",
      "--------------------------------------------------\n",
      "('Crooked Hillary Clinton is \"guilty as hell\" but the system is totally rigged and corrupt! Where are the 33,000 missing e-mails?', -13.0)\n",
      "**************************************************\n",
      "('My heart &amp; prayers go out to all of the victims of the terrible #Brussels tragedy. This madness must be stopped, and I will stop it.', -13.0)\n",
      "**************************************************\n",
      "('Crooked Hillary Clinton is a fraud who has put the public and country at risk by her illegal and very stupid use of e-mails. Many missing!', -13.0)\n",
      "**************************************************\n",
      "('My lawyers want to sue the failing @nytimes so badly for irresponsible intent. I said no (for now), but they are watching. Really disgusting', -12.0)\n",
      "**************************************************\n",
      "('Hillary Clinton is unfit to be president. She has bad judgement, poor leadership skills and a very bad and destructive track record. Change!', -12.0)\n",
      "**************************************************\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"TRUMP, LOWEST POLARITY\")\n",
    "print(\"-\"*50)\n",
    "trump = trump.sort_values(by='pscore')\n",
    "for index, row in trump.head().iterrows():\n",
    "    print((row['text'], row['pscore']))\n",
    "    print(\"*\"*50)\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `iterrows()` method:\n",
    "* Goes through each row and provides the index number and row data: for index, row\n",
    "* Print `text` of row and `pscore` of row where we are at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLINTON, LOWEST POLARITY\n",
      "--------------------------------------------------\n",
      "('It\\x92s wrong to tear each other down.\\r\\nIt\\x92s wrong to let income inequality get worse.\\r\\nIt\\x92s wrong to put a loose cannon in charge.', -12.0)\n",
      "**************************************************\n",
      "(\"Gun violence and hate aren't isolated\\x97homophobia in Orlando, racism in Charleston. We need to fight them together. https://t.co/HcBJzknBz7\", -12.0)\n",
      "**************************************************\n",
      "('There is something wrong with our country. There is too much violence...too much senseless killing, too many people dead who shouldn\\x92t be.', -11.0)\n",
      "**************************************************\n",
      "('We owe families of gun violence victims more than prayers.\\r\\n\\r\\nTell your senators to act on gun violence prevention: https://t.co/v0defjTptE', -11.0)\n",
      "**************************************************\n",
      "('RT @repjohnlewis: .@SpeakerRyan, we will not leave without acting for the victims &amp; families of reckless gun violence. #NoBillNoBreak https\\x85', -10.0)\n",
      "**************************************************\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"CLINTON, LOWEST POLARITY\")\n",
    "print(\"-\"*50)\n",
    "clinton = clinton.sort_values(by='pscore')\n",
    "for index, row in clinton.head().iterrows():\n",
    "    print((row['text'], row['pscore']))\n",
    "    print(\"*\"*50)\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lowest-scoring tweets for both candidates do seem to have very negative sentiment. \n",
    "\n",
    "## Select most positive tweets:\n",
    "* ``.head()`` method prints out the first five rows of the sorted data frame. \n",
    "* ``.tail()`` method prints out the last five rows of sorted DF with highest polarity scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRUMP, HIGHEST POLARITY\n",
      "--------------------------------------------------\n",
      "('People (pundits) gave me no chance in South Carolina. Now it looks like a possible win. I would be happy with a one vote victory! (HOPE)', 15.0)\n",
      "**************************************************\n",
      "('As expected, the media is very much against me. Their dishonesty is amazing but, just like our big wins in the primaries, we will win!', 15.0)\n",
      "**************************************************\n",
      "('\"@PaulaDuvall2: We\\'re all enjoying you, as well, Mr. T.! You\\'ve inspired Hope and a Positive Spirit throughout America! God bless you!\" Nice', 15.0)\n",
      "**************************************************\n",
      "('Fun to watch the Democrats working so hard to win the great State of South Carolina when I just won the Republican version - amazing people!', 17.0)\n",
      "**************************************************\n",
      "('Great honor to be endorsed by popular &amp; successful @gov_gilmore of VA. A state that I very much want to win-THX Jim! https://t.co/x4Y1TAFHvn', 18.0)\n",
      "**************************************************\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"TRUMP, HIGHEST POLARITY\")\n",
    "print(\"-\"*50)\n",
    "trump = trump.sort_values(by='pscore')\n",
    "for index, row in trump.tail().iterrows():\n",
    "    print((row['text'], row['pscore']))\n",
    "    print(\"*\"*50)\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CLINTON, HIGHEST POLARITY\n",
      "--------------------------------------------------\n",
      "('\\x93I married my best friend. I was still in awe of\\x85how smart and strong and loving and caring she was.\\x94 \\x97@BillClinton on Hillary', 11.0)\n",
      "**************************************************\n",
      "(\"Our first-ever gold medal-winning women's gymnastics team: the Magnificent Seven. https://t.co/MVkhHBh7hI\", 12.0)\n",
      "**************************************************\n",
      "(\"Happy Fourth of July! Today let's celebrate the best of America\\x97our freedom, diversity, and the values we share. -H https://t.co/C6Mdt4iVg5\", 12.0)\n",
      "**************************************************\n",
      "('The people taking care of our children and our parents deserve a good wage, good benefits, and a secure retirement.', 12.0)\n",
      "**************************************************\n",
      "(\"RT @clairecmc: He's humble.He's incredibly smart.He's a joyful warrior,a kind &amp; thoughtful person.He'll be a wonderful VP! Congrats to my f\\x85\", 15.0)\n",
      "**************************************************\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"CLINTON, HIGHEST POLARITY\")\n",
    "print(\"-\"*50)\n",
    "clinton = clinton.sort_values(by='pscore')\n",
    "for index, row in clinton.tail().iterrows():\n",
    "    print((row['text'], row['pscore']))\n",
    "    print(\"*\"*50)\n",
    "print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look at distribution using boxplot \n",
    "* Create boxplot visualization to show ``pscore`` for the tweets\n",
    "* One boxplot per candidate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAgEgQ8AgSDwASAQBD4ABILAB4BA/D9WxcIRZVGqEAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11ca292e8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.boxplot([clinton['pscore'], trump['pscore']], labels=[\"Clinton\", \"Trump\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In aggregate, both candidates have average around 0\n",
    "* Trump tweets distribution has a wider spread (more positive, more negative) \n",
    "* Clinton's tweets are more reserved when it comes to both positive and negative sentiment.\n",
    "\n",
    "## Summary of Knowledge-based sentiment analysis\n",
    "* We rely on an external knowledge base to help us determine the sentiment of Tweets. \n",
    "* The quality of our sentiment judgements, of course, is reliant on the quality of our knowledge base. \n",
    "* Other options\n",
    "\n",
    "1. AFINN (used above)\n",
    "2. [General Inquirer](http://www.wjh.harvard.edu/~inquirer/)\n",
    "3. [Liu Bing's Sentiment Lexicons](https://www.cs.uic.edu/~liub/FBS/sentiment-analysis.html)\n",
    "4. [Sentiwordnet](http://sentiwordnet.isti.cnr.it/), included in NLTK\n",
    "\n",
    "\n",
    "## Make your own sentiment vocabulary using \"seeds\"\n",
    "* Tweets from specialized domain (e.g., politics, product reviews) may vary from general use\n",
    "* If no premade lexicon is available, you can *make* a sentiment lexicon manually:\n",
    "\n",
    "### Process introduced by Turney 2002 (http://www.aclweb.org/anthology/P02-1053.pdf)\n",
    "* Start with small set of *seed words* that are positive or negative, and find words strongly associated with seeds. \n",
    "* E.g., if a word consistently appears with word *bad*, it must have some negative sentiment. \n",
    "* The negativity or positivity from the seed words is propogated to other words in dataset. \n",
    "\n",
    "\n",
    "## Pointwise Mutual Information (PMI)\n",
    "* Measurement of how strongly associated two words are given a corpus and a unit of analysis. * Ratio of how often two words occur together vs. how often we expect them to occur together\n",
    "\n",
    "$$\n",
    "\\text{PMI}(a, b) = \\text{log}\\frac{P(a, b)}{P(a) \\cdot P(b)}\n",
    "$$\n",
    "\n",
    "* P(a, b) is probability that both a and b appear in same unit of analysis (sentence, tweet, doc)\n",
    "* P(a) is probability that a appears in a unit and \n",
    "* P(b) is probability that b appears in a unit. \n",
    "\n",
    "### PMI intuition \n",
    "* If two words appear together more often than we would expect them to appear by chance, then they are strongly associated. \n",
    "* Numerator represents how likely they are to appear together, \n",
    "* Denominator represents how often we expect them appear together by chance. \n",
    "\n",
    "If we take Tweets to be our unit of analysis:\n",
    "\n",
    "* $P(a, b)$ is simply the number of Tweets that a and b appear together divided by the total number of tweets\n",
    "* $P(a)$ is the number of Tweets that a appears in, divided by the total number of tweets\n",
    "* $P(b)$ is the number of Tweets that b appears in, divided by the total number of tweets\n",
    "\n",
    "### Semantic Orientation of a Word\n",
    "* Start with a set of positive and negative seed words. \n",
    "* Call positive seed words $V^{+}$ and negative seed words $V^{-}$. \n",
    "* Add together its PMIs with the positive words, subtracct the PMIs with negative word\n",
    "\n",
    "$$\n",
    "V^{+} = \\{\\text{good}, \\text{great}, \\text{better}\\} \\\\\n",
    "V^{-} = \\{\\text{bad}, \\text{terrible}, \\text{worse}\\}\n",
    "$$\n",
    "\n",
    "The semantic orientation of a word is simply the sum of its PMIs with the positive seed words, minus the sum of its PMIs with negative seed words. \n",
    "\n",
    "$$\n",
    "\\text{SO}(w) = \\sum_{v^{+} \\in V^{+}} \\text{PMI}(w, v^{+}) - \\sum_{v^{-} \\in V^{-}} \\text{PMI}(w, v^{-})\n",
    "$$\n",
    "\n",
    "Once you have the semantic orientation for all the words in your corpus, you have something similar to the AFINN lexicon - each word has a positive or negative word associated it. From there, you can iterate through all your tweets. \n",
    "\n",
    "The step-by-step process of getting from a corpus and seed words to a list of words with semantic orientations is pretty complex, so I've written a function here that will do the work for you. You can copy and paste this code into your own if you wish to take this approach. \n",
    "\n",
    "## Function ``getso`` takes four arguments:\n",
    "* ``seed_pos``: a list containing the positive seed words\n",
    "* ``seed_neg``: a list containing the negative seed words\n",
    "* ``texts``: an iterable containing the raw texts\n",
    "* ``min_df``: an integer, meaning \"minimum document frequency\". This tells the function to ignore all words that appear less than ``min_df`` times in the corpus. This will remove outliers that appear infrequently and therefore are noise.\n",
    "\n",
    "### The function returns a dictionary. \n",
    "* The keys of the dictionary are words, \n",
    "* their corresponding values are the SO calculated for that word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def getso(seed_pos, seed_neg, texts, min_df):\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    from sklearn.preprocessing import binarize\n",
    "    import numpy as np\n",
    "    n = len(texts)\n",
    "    cv = CountVectorizer(min_df=min_df)\n",
    "    vector = cv.fit_transform(texts)\n",
    "    rvocab = {v: k for k, v in cv.vocabulary_.items()}\n",
    "    bvector = binarize(vector)\n",
    "    docfreq = np.sum(bvector.toarray(), axis=0)\n",
    "    docfreq = docfreq/n\n",
    "    docfreq = docfreq.reshape(docfreq.shape[0], 1)\n",
    "    com = vector.T.dot(vector).toarray()\n",
    "    np.fill_diagonal(com, 0)\n",
    "    com = com/n\n",
    "    p_single = docfreq.dot(docfreq.T)\n",
    "    pmi = np.divide(com, p_single)\n",
    "    sos = {}\n",
    "    for word, index in cv.vocabulary_.items():\n",
    "        so = 0\n",
    "        for pos in seed_pos:\n",
    "            p_index = cv.vocabulary_[pos]\n",
    "            so += pmi[index, p_index]\n",
    "        for neg in seed_neg:\n",
    "            n_index = cv.vocabulary_[neg]\n",
    "            so -= pmi[index, n_index]\n",
    "        sos[word] = so\n",
    "    return sos  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take seed words and tweet texts and feed them to this new function. \n",
    "* This will result in a dictionary of semantic orientations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed_pos = [\"good\", \"great\", \"better\"]\n",
    "seed_neg = [\"bad\", \"terrible\", \"worse\"]\n",
    "texts = df['text']\n",
    "so = getso(seed_pos, seed_neg, texts, min_df=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are semantic orientation of words \"hillary\", \"trump\", \"economy\" and \"terrorist\"?\n",
    "* \"hillary\" and \"economy\" have slightly negative orientations, \n",
    "* \"trump\" has a slightly positive one, \n",
    "* while \"terrorist\" has a very negative one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-2.74509595015\n",
      "0.309027941379\n",
      "-4.85461285009\n",
      "-13.9458884329\n"
     ]
    }
   ],
   "source": [
    "print(so['hillary'])\n",
    "print(so['trump'])\n",
    "print(so['economy'])\n",
    "print(so['terrorist'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How effective are our semantic orientations? \n",
    "1. Only 5000 tweets, need more data (equal numbers of tweets from both candidates) \n",
    "2. Quality of seed words matters \n",
    "3. Trump twitter account tends to use more adjectives and seems to influences the semantic orientation outcome slightly more. \n",
    "\n",
    "Here's a look at the most positive and negative words, by semantic orientation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('barriers', 23.076923076923077),\n",
       " ('david', 19.230769230769234),\n",
       " ('benefits', 18.641671466722524),\n",
       " ('politics', 17.618200783461127),\n",
       " ('paying', 16.132720495758157),\n",
       " ('facing', 15.384615384615387),\n",
       " ('ourselves', 13.986013986013987),\n",
       " ('wages', 13.784887678692993),\n",
       " ('do', 13.613680347275036),\n",
       " ('deserve', 13.206262763784888),\n",
       " ('than', 13.03075976803669),\n",
       " ('biggest', 13.008824149662459),\n",
       " ('evening', 12.798415311792549),\n",
       " ('press', 12.69122590382109),\n",
       " ('michael', 12.582938164333511),\n",
       " ('stay', 12.302350342782848),\n",
       " ('looking', 12.254336360805459),\n",
       " ('courage', 12.117086453369641),\n",
       " ('grow', 12.117086453369641),\n",
       " ('tuesday', 11.608402707999575),\n",
       " ('rnc', 11.568276684555755),\n",
       " ('face', 11.538461538461538),\n",
       " ('hate', 11.406153410681092),\n",
       " ('spending', 11.25049678976292),\n",
       " ('afternoon', 11.223922008033966),\n",
       " ('higher', 11.117736699132049),\n",
       " ('job', 11.068784712254704),\n",
       " ('children', 11.067585144041205),\n",
       " ('since', 10.82497763083839),\n",
       " ('polls', 10.6869941500645),\n",
       " ('100', 10.210465028328787),\n",
       " ('able', 10.14154718620658),\n",
       " ('problems', 10.097572044474703),\n",
       " ('clear', 9.6153846153846168),\n",
       " ('especially', 9.4216455607688374),\n",
       " ('heading', 9.3667051888908244),\n",
       " ('tampa', 9.3420791095209701),\n",
       " ('democrats', 9.3242810915668279),\n",
       " ('politicians', 9.2647805826100402),\n",
       " ('path', 9.2195883510968919),\n",
       " ('listen', 9.1899251191286595),\n",
       " ('any', 9.0878148400272281),\n",
       " ('apart', 8.9842973563903801),\n",
       " ('profits', 8.8495575221238933),\n",
       " ('wage', 8.8495575221238933),\n",
       " ('congrats', 8.8495575221238933),\n",
       " ('place', 8.7723567177501458),\n",
       " ('honor', 8.704853438095574),\n",
       " ('lead', 8.5801364871132311),\n",
       " ('please', 8.5684270278050167)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(sorted(so.items(), key=lambda x:x[1], reverse=True))[:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('judgement', -65.36172161172162),\n",
       " ('tpp', -46.914623837700759),\n",
       " ('getting', -46.800473041297629),\n",
       " ('growth', -46.27529554855137),\n",
       " ('reporting', -41.666666666666671),\n",
       " ('dead', -40.700404743338552),\n",
       " ('china', -38.224318747574571),\n",
       " ('nafta', -35.880729209524304),\n",
       " ('crime', -35.305535201347098),\n",
       " ('victims', -34.722222222222221),\n",
       " ('attacks', -32.738095238095241),\n",
       " ('san', -32.274895646988675),\n",
       " ('voted', -31.918579208402218),\n",
       " ('wrote', -30.885780885780882),\n",
       " ('zero', -29.737379197144584),\n",
       " ('possible', -28.733839987992816),\n",
       " ('mess', -28.694756570862765),\n",
       " ('which', -27.485816210669576),\n",
       " ('held', -26.327838827838832),\n",
       " ('divided', -26.069726733443552),\n",
       " ('anti', -25.635794913640694),\n",
       " ('crazy', -25.248835158010753),\n",
       " ('course', -23.708010335917315),\n",
       " ('income', -22.727272727272727),\n",
       " ('call', -22.588522588522586)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(sorted(so.items(), key=lambda x:x[1]))[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some the negative words do seem to be appropriately marked (dead, crime, victims, attacks, mess), but others seem to again be indicative of a bias towards Trump (due to negative marks for words like \"TPP\" and \"China\"). The words classified as highly positive aren't exactly of high quality. Words like barrier, facing, and David are scored as being highly positive.\n",
    "\n",
    "There are a few of reasons for this unimpressive result:\n",
    "\n",
    "* As I mentioned, for the task of automatic lexicon generation, this is a pretty small dataset. There really isn't enough text here to demonstrate that two given words have a strong relationship. If you use this approach, try to have more than 5000 texts available to you.\n",
    "* The quality of automatic lexicon generation depends very much on the words you choose as positive and negative seeds. I just \"guessed\" words such as \"good\", \"great\", and \"better\" to be positive words, but it would probably be better for you to do a systematic exploration of your data to find some good seed words. Choosing *more* seed words would also help.\n",
    "\n",
    "Assuming you do get a decent lexicon from this automatic process, you can then take a route similar with AFINN; go through each tweet, add up the SO scores for each word in the tweet, and you have one number representing the polarity of the tweet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised vs. Unsupervised\n",
    "* Unsupervised task: unlabeled data from *external knowledge base* `AFINN` to assign quantitative measure of polarity. \n",
    "* Drawback of unsupervised learning: no systematic way to evaluate it. \n",
    "* Supervised task: train classifier on labeled tweets, evaluate sentiment, but we don't know whether a Tweet is positive or negative -\n",
    "\n",
    "## How to get to supervised learning?\n",
    "* Labels from previous examples: Iris species, Tweet from iPhone/Android. \n",
    "\n",
    "### 1. Manually Labeling a set of Tweets\n",
    "* Subset tweets and manually identify them as positive or negative is difficult and tedious work, but someone eventually has to do it. \n",
    "* Best practice: multiple people annotate tweets and compare results between different coders for \"Inter-rate Reliability\" \n",
    "* Amazon Mechanical Turk \"crowdsourcing\" to annotate the tweets,  \n",
    "\n",
    "## 2. Use Existing dataset\n",
    "* Labeled dataset relevant to your research interest may already exist. \n",
    "* Train classifier on large sample assuming high-quality annotations.\n",
    "\n",
    "## 3. Use `proxy` in data for sentiment\n",
    "* Find indicator in data as unambiguous proxy for pos./neg. sentiment. \n",
    "* Tweets with smile emoticon, \":)\", are positive, sad emoticon, \":(\", is a negative one. \n",
    "* Train classifier on subset of tweets, use model to classify tweets with no emoticons.\n",
    "\n",
    "# Example: Supervised Learning \n",
    "* Train sentiment classifier using combination of approaches 2 and 3 described above. \n",
    "* `Sentiment140` classifier allows you discover the sentiment of a brand, product, or topic on Twitter. \n",
    "* Dataset containing 1,600,000 tweets ending in either a smiling emoticon \":)\" or a negative emoticon \":(\". \n",
    "* Positive tweets were assigned label of 4, negative tweets label of 0. \n",
    "\n",
    "## Data set saved in .csv format on Canvas. \n",
    "* Import data frame to python using Pandas \n",
    "* Fit classifier model on training data\n",
    "* Predict test data with classifier model: Clinton/Trump tweets.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = pd.read_csv(\"training.1600000.processed.noemoticon.csv\", names=[\"polarity\", \"tid\", \"date\", \"query\", \"user\", \"text\"], encoding=\"latin1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Represent tweet texts as vector using TFIDF vectorizer. \n",
    "* Call named argument ``min_df`` with value of 10 to tell the vectorizer to ignore all words that occur in fewer than 10 tweets. \n",
    "* Words that occur infrequently don't have much statistical weight but also would increase dimensionality of vectors, so we ignore them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tv = TfidfVectorizer(min_df=50)\n",
    "X = tv.fit_transform(training['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600000, 13451)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Data\n",
    "* X matrix has 1,600,000 rows (one for each sample) and 43,448 columns, with 43,448 unique words in dataset that occur in 10 or more tweets. \n",
    "* Get polarity column from data to serve as our labels in variable ``y``. \n",
    "* Shuffle data using ``shuffle``, in case data is stored in specific order\n",
    "\n",
    "## Use subset of Training data\n",
    "* Training a model on 1,600,000 samples would take a long time. \n",
    "* Obtain 500,000 tweets to shorten training time (instruction purposes) \n",
    "* Otherwise, train your model on as many data points as possible, regardless of time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = training['polarity']\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "X, y = shuffle(X, y)\n",
    "\n",
    "X, y = X[:50000], y[:50000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use SkLearn in familiar fashion. \n",
    "* Train linear SVM on the data. \n",
    "* Split data into train and test sets \n",
    "* Use held-out test data to evaluate model performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LibSVM]"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape=None, degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=3)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=3057)\n",
    "clf = SVC(kernel=\"linear\", verbose=3)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.77464"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Model Performance\n",
    "* Accuracy of SVM model is 77% on held out data.\n",
    "\n",
    "## Use trained classifier model to predict new test values\n",
    "* Convert data into same vector representations used to train classifier\n",
    "* Use TFIDF vectorizer used on training data, saved as variable ``tv``. \n",
    "* Use ``.transform()`` method  **instead** of ``.fit_transform()``. \n",
    "\n",
    "### FIDF Vectorizer has already been fit to the training data. \n",
    "* Transform our Tweet texts into TF-IDF representation and not fit the vectorizer to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_tfidf = tv.transform(df['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict already-trained classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweet_pred = clf.predict(tweet_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add predictions of classifier to data frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[\"sent_pred\"] = tweet_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print first 50 Tweets and label assigned by classifier\n",
    "* 0 means negative and 4 means positive. \n",
    "* Not perfect, but decent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Remember, don\\'t believe \"sources said\" by the VERY dishonest media. If they don\\'t name the sources, the sources don\\'t exist.', 0)\n",
      "**************************************************\n",
      "('Did Crooked Hillary help disgusting (check out sex tape and past) Alicia M become a U.S. citizen so she could use her in the debate?', 4)\n",
      "**************************************************\n",
      "('Using Alicia M in the debate as a paragon of virtue just shows that Crooked Hillary suffers from BAD JUDGEMENT! Hillary was set up by a con.', 4)\n",
      "**************************************************\n",
      "('Wow, Crooked Hillary was duped and used by my worst Miss U. Hillary floated her as an \"angel\" without checking her past, which is terrible!', 0)\n",
      "**************************************************\n",
      "('Anytime you see a story about me or my campaign saying \"sources said,\" DO NOT believe it. There are no sources, they are just made up lies!', 4)\n",
      "**************************************************\n",
      "('Wow, did you see how badly @CNN (Clinton News Network) is doing in the ratings. With people like @donlemon, who could expect any more?', 4)\n",
      "**************************************************\n",
      "('While Hillary profits off the rigged system, I am fighting for you! Remember the simple phrase: #FollowTheMoney\\x85 https://t.co/8mVInc82E9', 4)\n",
      "**************************************************\n",
      "('Thank you for joining me this afternoon, New Hampshire! Will be back soon. #FollowTheMoney\\r\\nSpeech transcript:\\x85 https://t.co/VtUkDgF4vs', 4)\n",
      "**************************************************\n",
      "('Join me in Manheim, Pennsylvania on Saturday at 7pm! #TrumpRally\\r\\nTickets: https://t.co/ADOGW34ctF https://t.co/LNWQZ9yUJy', 4)\n",
      "**************************************************\n",
      "(\"My condolences to those involved in today's horrible accident in NJ and my deepest gratitude to all of the amazing first responders.\", 0)\n",
      "**************************************************\n",
      "('Will be in Novi, Michigan this Friday at 5:00pm. Join the MOVEMENT! Tickets available at: https://t.co/Q6APf0ZFYA\\x85 https://t.co/6WAyO9eQHN', 0)\n",
      "**************************************************\n",
      "(\"Join me in Bedford, New Hampshire- tomorrow at 3:00pm. Can't wait to see everyone! #AmericaFirst #MAGA\\x85 https://t.co/oeOJFAS7it\", 4)\n",
      "**************************************************\n",
      "('Thank you Waukesha, Wisconsin! \\r\\nFull transcript of my speech, #FollowTheMoney:\\r\\nhttps://t.co/Xb1yyDSNNf https://t.co/WdKK6nJCZW', 4)\n",
      "**************************************************\n",
      "('Joining @oreillyfactor from Waukesha, Wisconsin - now, live! Enjoy!', 4)\n",
      "**************************************************\n",
      "('Join me live in Waukesha, Wisconsin for an 8pmE rally! #AmericaFirst #MAGA\\r\\nhttps://t.co/G8kGLSFy6S', 4)\n",
      "**************************************************\n",
      "('Thank you Council Bluffs, Iowa! Will be back soon. Remember- everything you need to know about Hillary -- just\\x85 https://t.co/45kIHxdX83', 4)\n",
      "**************************************************\n",
      "('RT @TeamTrump: \"She put the office of Sec of State up for sale. If she ever got the chance, she\\x92d put the Oval Office up for sale too.\" #Fo\\x85', 4)\n",
      "**************************************************\n",
      "('An honor to meet with the Polish American Congress in Chicago this morning! #ImWithYou \\r\\nVideo:\\x85 https://t.co/lBFHoWRqox', 0)\n",
      "**************************************************\n",
      "('Melania and I extend our deepest condolences to the family of Shimon Peres...https://t.co/xeGYL2IzUP', 0)\n",
      "**************************************************\n",
      "('Join me in Council Bluffs, Iowa- today at 3pm! #MakeAmericaGreatAgain \\r\\nTickets: https://t.co/iRL3xh37gF', 4)\n",
      "**************************************************\n",
      "('Every on-line poll, Time Magazine, Drudge etc., has me winning the debate. Thank you to Fox &amp; Friends for so reporting!', 4)\n",
      "**************************************************\n",
      "('My supporters are the best! $18 million from hard-working people who KNOW what we can be again! Shatter the record: https://t.co/8ZHGyOth0f', 0)\n",
      "**************************************************\n",
      "('Unbelievable evening in Melbourne, Florida w/ 15,000 supporters- and an additional 12,000 who could not get in. Tha\\x85 https://t.co/VU5wh2zXBU', 0)\n",
      "**************************************************\n",
      "('Join me for a 3pm rally - tomorrow at the Mid-America Center in Council Bluffs, Iowa! Tickets:\\x85 https://t.co/dfzsbICiXc', 4)\n",
      "**************************************************\n",
      "('Once again, we will have a government of, by and for the people. Join the MOVEMENT today! https://t.co/lWjYDbPHav https://t.co/uYwJrtZkAe', 4)\n",
      "**************************************************\n",
      "(\"RT @GOP: On National #VoterRegistrationDay, make sure you're registered to vote so we can #MakeAmericaGreatAgain https://t.co/GKcaLkx8C8 ht\\x85\", 4)\n",
      "**************************************************\n",
      "(\"Hillary Clinton's Campaign Continues To Make False Claims About Foundation Disclosure: \\r\\nhttps://t.co/zhkEfUouHH\", 4)\n",
      "**************************************************\n",
      "(\"'CNBC, Time magazine online polls say Donald Trump won the first presidential debate' via @WashTimes. #MAGA\\r\\nhttps://t.co/PGimqYKPoJ\", 4)\n",
      "**************************************************\n",
      "('Great afternoon in Little Havana with Hispanic community leaders. Thank you for your support! #ImWithYou https://t.co/vxWZ2tyJTF', 4)\n",
      "**************************************************\n",
      "('In the last 24 hrs. we have raised over $13M from online donations and National Call Day, and we\\x92re still going! Thank you America! #MAGA', 4)\n",
      "**************************************************\n",
      "(\"Well, now they're saying that I not only won the NBC Presidential Forum, but last night the big debate. Nice!\", 0)\n",
      "**************************************************\n",
      "('Thank you for your endorsement, @GovernorSununu. #MAGA \\r\\nhttps://t.co/8BEeQPsuyd', 4)\n",
      "**************************************************\n",
      "('Such a great honor. Final debate polls are in - and the MOVEMENT wins!\\r\\n#AmericaFirst #MAGA #ImWithYou\\x85 https://t.co/DV1BKMwHEM', 4)\n",
      "**************************************************\n",
      "(\"'U.S. Murders Increased 10.8% in 2015' via @WSJ: https://t.co/CIJMQJhLqp\", 4)\n",
      "**************************************************\n",
      "('Thank you! #TrumpWon #MAGA \\r\\nhttps://t.co/a5rr1i38km', 4)\n",
      "**************************************************\n",
      "(\"Hillary's been failing for 30 years in not getting the job done - it will never change.\", 0)\n",
      "**************************************************\n",
      "(\"'True blue-collar billionaire Donald Trump shows Hillary Clinton is out of touch' https://t.co/NHO1OicfVm\", 4)\n",
      "**************************************************\n",
      "('The #1 trend on Twitter right now is #TrumpWon - thank you!', 4)\n",
      "**************************************************\n",
      "('I won every poll from last nights Presidential Debate - except for the little watched @CNN poll.', 0)\n",
      "**************************************************\n",
      "(\"'How Trump won over a bar full of undecideds and Democrats'\\r\\nhttps://t.co/WWO39kxn8Y\", 0)\n",
      "**************************************************\n",
      "(\"I really enjoyed the debate last night.Crooked Hillary says she is going to do so many things.Why hasn't she done them in her last 30 years?\", 0)\n",
      "**************************************************\n",
      "('Great debate poll numbers - I will be on @foxandfriends at 7:00 to discuss. Enjoy!', 4)\n",
      "**************************************************\n",
      "('Thank you! Four new #DebateNight polls with the MOVEMENT winning. Together, we will MAKE AMERICA SAFE &amp; GREAT AGAIN\\x85 https://t.co/39FCnUf8Pb', 4)\n",
      "**************************************************\n",
      "(\".@DRUDGE_REPORT's First Presidential Debate Poll:\\r\\nTrump: 80%\\r\\nClinton: 20%\\r\\nJoin the MOVEMENT today &amp; lets #MAGA!\\x85 https://t.co/B12lgC97tn\", 4)\n",
      "**************************************************\n",
      "('Thank you! CNBC #DebateNight poll with over 400,000 votes. \\r\\nTrump 61%\\r\\nClinton 39%\\r\\n#AmericaFirst #ImWithYou\\x85 https://t.co/MJ3NwA98op', 4)\n",
      "**************************************************\n",
      "('TIME #DebateNight poll - over 800,000 votes. Thank you! \\r\\n#AmericaFirst #MAGA https://t.co/bTPX9E0wKu', 4)\n",
      "**************************************************\n",
      "('.@newtgingrich just said \"a historic victory for Trump.\" NICE!', 4)\n",
      "**************************************************\n",
      "(\"Wow, did great in the debate polls (except for @CNN - which I don't watch). Thank you!\", 4)\n",
      "**************************************************\n",
      "('Thank you Governor @TerryBranstad! \\r\\n#AmericaFirst #Debates2016 https://t.co/yIeZctdQy8', 4)\n",
      "**************************************************\n",
      "('Thank you Governor @Mike_Pence!\\r\\nLets MAKE AMERICA SAFE AND GREAT AGAIN with the American people. \\r\\n#AmericaFirst\\x85 https://t.co/6k7qP9X8nC', 4)\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "for index, row in df.head(50).iterrows():\n",
    "    print((row['text'], row['sent_pred']))\n",
    "    print(\"*\"*50)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
