{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Media Mining: Word and Characters\n",
    "### Vincent Malic - Spring 2018\n",
    "\n",
    "# Module 9.1  N-grams in sklearn\n",
    "* Work with small amount of fake data to illustrate, but nothing in principle prevents this from working with any set of text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"The quick brown fox jumps behind the lazy dog.\",\n",
    "    \"The lazy brown fox leaps beyond the sleeping dog.\",\n",
    "    \"This lazy dog has a Twitter account that the fox subscribes to.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the count vectorizer as usual.\n",
    "* Make texts turn into a count vector by initializing a Count Vectorizer.\n",
    "* Convert vector to array of words and counts for text\n",
    "* Results in a vector of 4 rows and 19 columns representing the words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "vectors = cv.fit_transform(texts)\n",
    "\n",
    "vectors = vectors.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 19)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 2, 0, 0, 0],\n",
       "       [0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 2, 0, 0, 0],\n",
       "       [0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 2, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vectorizer object attribute ``vocabulary_``\n",
    "* Contains information about *which element in a vector* corresponds to *which word* is in the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 15, 'quick': 11, 'brown': 3, 'fox': 5, 'jumps': 7, 'over': 10, 'lazy': 8, 'dog': 4, 'behind': 1, 'leaps': 9, 'beyond': 2, 'sleeping': 12, 'this': 16, 'has': 6, 'twitter': 18, 'account': 0, 'that': 14, 'subscribes': 13, 'to': 17}\n"
     ]
    }
   ],
   "source": [
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word count by position in text\n",
    "* The 0th element of first count vector has a value of 0, we know the word \"account\" appears in it 0 times. \n",
    "* The 1st element of the first count vector has a value of 1, so we know \"brown\" appears in it 1 time.\n",
    "* Can modify how Count Vectorizer does its vectorization by *passing parameters to factory method during initialization*.\n",
    "\n",
    "## **Presence vectors** versus **Count vectors**\n",
    "* Some of readings have shown that presence vectors perform better than count vectors\n",
    "* In **presence vector** value 1 indicates the word is in the text, 0 means its absence.\n",
    "* factory method has named argument called ``binary`` that if True, produces presence vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 19)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0],\n",
       "       [1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(binary=True)\n",
    "vectors = cv.fit_transform(texts)\n",
    "print(vectors.shape)\n",
    "vectors.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Presence vectors only have values of 0 and 1.\n",
    "* In sample sentences, some words appeared twice in same text, represented by a 2 in Count Vectors.\n",
    "\n",
    "\n",
    "# N-Grams\n",
    "* N-gram is combination of n consecutive words. \n",
    "* In first sentence: (\"the\", \"quick\") is a bigram, and (\"quick\", \"brown\") is a bigram. \n",
    "* Considering n-grams *preserves some information in order of the words*. \n",
    "\n",
    "### For example, with a bigram \n",
    "* Classifier can know that \"quick\" and \"brown\" occurred together.\n",
    "* Cost of *many combinations of 2 words*: length of vector becomes *much longer*. \n",
    "* Vector length increases for 3-grams or 4-grams, but may be *sparse* as combinations of words can be more rare.\n",
    "\n",
    "## Parameter `ngram_range` \n",
    "* Considers which n-grams to count: default value is ``(1, 1)``, e.g., unigrams. \n",
    "* ngram_range of ``(2, 2)``, considers only bigrams, count every combination of two words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 25)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1,\n",
       "        0, 0, 0],\n",
       "       [0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1,\n",
       "        0, 0, 0],\n",
       "       [0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0,\n",
       "        1, 0, 0],\n",
       "       [1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
       "        0, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(ngram_range=(2, 2))\n",
    "bigram_vectors = cv.fit_transform(texts)\n",
    "print(bigram_vectors.shape)\n",
    "bigram_vectors.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: Dimensionality has increased. \n",
    "* There are 19 unique words, but there are 25 unique bigrams.\n",
    "\n",
    "## Peek into the vocabulary: \n",
    "* Algorithm is counting combinations of two words, instead of single words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'account that': 0,\n",
       " 'behind the': 1,\n",
       " 'beyond the': 2,\n",
       " 'brown fox': 3,\n",
       " 'dog has': 4,\n",
       " 'fox jumps': 5,\n",
       " 'fox leaps': 6,\n",
       " 'fox subscribes': 7,\n",
       " 'has twitter': 8,\n",
       " 'jumps behind': 9,\n",
       " 'jumps over': 10,\n",
       " 'lazy brown': 11,\n",
       " 'lazy dog': 12,\n",
       " 'leaps beyond': 13,\n",
       " 'over the': 14,\n",
       " 'quick brown': 15,\n",
       " 'sleeping dog': 16,\n",
       " 'subscribes to': 17,\n",
       " 'that the': 18,\n",
       " 'the fox': 19,\n",
       " 'the lazy': 20,\n",
       " 'the quick': 21,\n",
       " 'the sleeping': 22,\n",
       " 'this lazy': 23,\n",
       " 'twitter account': 24}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizer that considers *both* unigrams and bigrams:\n",
    "* Using factory method, we designate `ngram_range` with tuple (1, 2)\n",
    "* e.g. Minimum is a 1-gram, and maximum is a 2-gram\n",
    "* Dimensionality has increased markedly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 44)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1,\n",
       "        0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 2, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1,\n",
       "        0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 2, 0, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
       "        1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 2, 0, 1, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1]], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(ngram_range=(1, 2))\n",
    "ngram_vectors = cv.fit_transform(texts)\n",
    "print(ngram_vectors.shape)\n",
    "ngram_vectors.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Considering both one-grams and bigrams has increased our dimensionality to 44.\n",
    "* Element in position 1 represents the bigram, \"account that\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'account': 0,\n",
       " 'account that': 1,\n",
       " 'behind': 2,\n",
       " 'behind the': 3,\n",
       " 'beyond': 4,\n",
       " 'beyond the': 5,\n",
       " 'brown': 6,\n",
       " 'brown fox': 7,\n",
       " 'dog': 8,\n",
       " 'dog has': 9,\n",
       " 'fox': 10,\n",
       " 'fox jumps': 11,\n",
       " 'fox leaps': 12,\n",
       " 'fox subscribes': 13,\n",
       " 'has': 14,\n",
       " 'has twitter': 15,\n",
       " 'jumps': 16,\n",
       " 'jumps behind': 17,\n",
       " 'jumps over': 18,\n",
       " 'lazy': 19,\n",
       " 'lazy brown': 20,\n",
       " 'lazy dog': 21,\n",
       " 'leaps': 22,\n",
       " 'leaps beyond': 23,\n",
       " 'over': 24,\n",
       " 'over the': 25,\n",
       " 'quick': 26,\n",
       " 'quick brown': 27,\n",
       " 'sleeping': 28,\n",
       " 'sleeping dog': 29,\n",
       " 'subscribes': 30,\n",
       " 'subscribes to': 31,\n",
       " 'that': 32,\n",
       " 'that the': 33,\n",
       " 'the': 34,\n",
       " 'the fox': 35,\n",
       " 'the lazy': 36,\n",
       " 'the quick': 37,\n",
       " 'the sleeping': 38,\n",
       " 'this': 39,\n",
       " 'this lazy': 40,\n",
       " 'to': 41,\n",
       " 'twitter': 42,\n",
       " 'twitter account': 43}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Tokenization\n",
    "* ``CountVectorizer`` is counting instances of words, of n-grams inside of a string \n",
    "* First, it has to split the sentence into a list of strings, and then count the words\n",
    "* Sometimes the default tokenizer may not always be sufficient \n",
    "\n",
    "### Some situations call for finer-grained control of tokenization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'annaschneider': 0,\n",
       " 'be': 1,\n",
       " 'hopefully': 2,\n",
       " 'in': 3,\n",
       " 'interested': 4,\n",
       " 'joeschmoe': 5,\n",
       " 'lessons': 6,\n",
       " 'll': 7,\n",
       " 'more': 8,\n",
       " 'on': 9,\n",
       " 'python': 10,\n",
       " 'really': 11,\n",
       " 'rt': 12,\n",
       " 'thanks': 13,\n",
       " 'the': 14,\n",
       " 'there': 15,\n",
       " 'tutorial': 16,\n",
       " 'way': 17,\n",
       " 'your': 18}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweettext = [\n",
    "    \"RT @joeschmoe I'm really interested in your tutorial on #python!\",\n",
    "    \"@annaschneider Thanks! Hopefully there'll be more #python lessons on the way!\"]\n",
    "cv = CountVectorizer()\n",
    "cv.fit_transform(tweettext)\n",
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the ``sklearn`` Count Vectorizer ignored \"RT\" and stripped the @ sign from the mention and the # sign from the retweet. \n",
    "\n",
    "## NLTK Tweet Tokenizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RT',\n",
       " '@joeschmoe',\n",
       " \"I'm\",\n",
       " 'really',\n",
       " 'interested',\n",
       " 'in',\n",
       " 'your',\n",
       " 'tutorial',\n",
       " 'on',\n",
       " '#python',\n",
       " '!']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "tt = TweetTokenizer()\n",
    "tt.tokenize(tweettext[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If we want CountVectorizer object to use a *different tokenizer* \n",
    "* Specify parameter called ``tokenizer`` and \n",
    "* Pass the name of the function we want to use to tokenize*.\n",
    "* Normally, if tokenizing a single text with the initialized Tweet tokenizer, we'd type:\n",
    "\n",
    "```python\n",
    "tt.tokenize(sometext)\n",
    "```\n",
    "\n",
    "When we want to pass it to a Count Vectorizer object, we use the parameter ``tokenizer=tt.tokenize``."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(tokenizer=tt.tokenize)\n",
    "\n",
    "# CV will now use tt.tokenize() to tokenize a text it receives.\n",
    "tweetvector = cv.fit_transform(tweettext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1],\n",
       "       [2, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweetvector.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!': 0,\n",
       " '#python': 1,\n",
       " '@annaschneider': 2,\n",
       " '@joeschmoe': 3,\n",
       " 'be': 4,\n",
       " 'hopefully': 5,\n",
       " \"i'm\": 6,\n",
       " 'in': 7,\n",
       " 'interested': 8,\n",
       " 'lessons': 9,\n",
       " 'more': 10,\n",
       " 'on': 11,\n",
       " 'really': 12,\n",
       " 'rt': 13,\n",
       " 'thanks': 14,\n",
       " 'the': 15,\n",
       " \"there'll\": 16,\n",
       " 'tutorial': 17,\n",
       " 'way': 18,\n",
       " 'your': 19}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorizer kep the mentions and hashtags intact, \n",
    "* Using ``tt.tokenize`` function to split the text strings instead of the default tokenizer.\n",
    "\n",
    "\n",
    "# Character N-Grams in ``sklearn``:\n",
    "* A *character n-gram* **counts characters instead of words**. \n",
    "* In some situations, character n-grams provide better performance than word n-grams. \n",
    "* Doing counts of characters instead of words is just involves a different way of tokenizing the text. \n",
    "\n",
    "## Splitting by characters, instead of by words\n",
    "* Use ``tokenizer`` parameter and pass it a function that takes a string and splits it into all its characters.\n",
    "* Splitting a text string into its constituent characters by converting the string into a list directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['H', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd', '!']\n"
     ]
    }
   ],
   "source": [
    "astring = \"Hello world!\"\n",
    "characters = list(astring)\n",
    "print(characters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pass ``list`` function directly to Count Vectorizer's ``tokenizer`` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CountVectorizer(tokenizer=list)\n",
    "character_vectors = cv.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 8,  1,  1,  1,  1,  1,  3,  1,  1,  2,  1,  1,  1,  1,  1,  1,  4,\n",
       "         1,  1,  2,  1,  2,  2,  1,  1,  1,  1,  1],\n",
       "       [ 8,  1,  1,  2,  1,  2,  3,  1,  1,  3,  2,  1,  1,  1,  1,  2,  3,\n",
       "         1,  1,  1,  1,  2,  2,  0,  1,  1,  1,  1],\n",
       "       [ 8,  1,  2,  2,  0,  2,  6,  1,  2,  2,  1,  0,  0,  3,  0,  3,  4,\n",
       "         2,  0,  1,  2,  2,  0,  0,  1,  1,  2,  1],\n",
       "       [11,  1,  5,  2,  3,  1,  3,  1,  1,  4,  3,  0,  0,  1,  0,  1,  4,\n",
       "         0,  0,  2,  5,  9,  2,  0,  1,  1,  1,  1]], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "character_vectors.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " '.': 1,\n",
       " 'a': 2,\n",
       " 'b': 3,\n",
       " 'c': 4,\n",
       " 'd': 5,\n",
       " 'e': 6,\n",
       " 'f': 7,\n",
       " 'g': 8,\n",
       " 'h': 9,\n",
       " 'i': 10,\n",
       " 'j': 11,\n",
       " 'k': 12,\n",
       " 'l': 13,\n",
       " 'm': 14,\n",
       " 'n': 15,\n",
       " 'o': 16,\n",
       " 'p': 17,\n",
       " 'q': 18,\n",
       " 'r': 19,\n",
       " 's': 20,\n",
       " 't': 21,\n",
       " 'u': 22,\n",
       " 'v': 23,\n",
       " 'w': 24,\n",
       " 'x': 25,\n",
       " 'y': 26,\n",
       " 'z': 27}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resulting vector contain counts of characters, including the spaces and periods.\n",
    "\n",
    "\n",
    "# Character n-grams AND Word n-grams\n",
    "* Burger and colleagues used character n-grams and word n-grams together. \n",
    "* Considered all character n-grams from 1 to 5, and all word 1-grams and 2-grams.\n",
    "* They used **presence vectors** not count vectors, and we'll represent them the same way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectorizer = CountVectorizer(tokenizer=tt.tokenize, binary=True, ngram_range=(1, 2))\n",
    "character_vectorizer = CountVectorizer(tokenizer=list, binary=True, ngram_range=(1, 5))\n",
    "\n",
    "word_vectors = word_vectorizer.fit_transform(texts)\n",
    "character_vectors = character_vectorizer.fit_transform(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now have a matrix countaining word counts:\n",
    "* 4 rows, for each text, and 49 columns: 49 unique 1-grams and 2-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 49)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Also have a matrix countaining character counts:\n",
    "* 481 columns, unique character 1-grams, 2-grams, 3-grams, 4-grams, and 5-grams. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 481)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "character_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatenate these two matrices\n",
    "* Using ``hstack`` function (\"horizontal stack\") will glue the rows together\n",
    "* Resulting in single new matrix with 4 rows and (481+49) = 530 columns.\n",
    "* ``hstack`` takes a tuple as its argument, with matrices to be pasted together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 530)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.sparse import hstack\n",
    "vectors = hstack((word_vectors, character_vectors))\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Two separate vocabulary dictionaries - Now merged into one\n",
    "* One for word n-grams, one for the character n-grams.\n",
    "* n-gram dictionaries tell us which character corresponds to the 0th element of a vector, which character corresponds to the 1st element of a vector\n",
    "* After concatenated the vectors, these **positions no longer hold**.\n",
    "\n",
    "# Single merged matrix for character vectors and word vectors\n",
    "* Word vector dictionary is correct (positions didn't change);\n",
    "* Need to update the character n-grams dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 0,\n",
       " '  a': 1,\n",
       " '  a  ': 2,\n",
       " '  a   t': 3,\n",
       " '  a   t w': 4,\n",
       " '  a c': 5,\n",
       " '  a c c': 6,\n",
       " '  a c c o': 7,\n",
       " '  b': 8,\n",
       " '  b e': 9,\n",
       " '  b e h': 10,\n",
       " '  b e h i': 11,\n",
       " '  b e y': 12,\n",
       " '  b e y o': 13,\n",
       " '  b r': 14,\n",
       " '  b r o': 15,\n",
       " '  b r o w': 16,\n",
       " '  d': 17,\n",
       " '  d o': 18,\n",
       " '  d o g': 19,\n",
       " '  d o g  ': 20,\n",
       " '  d o g .': 21,\n",
       " '  f': 22,\n",
       " '  f o': 23,\n",
       " '  f o x': 24,\n",
       " '  f o x  ': 25,\n",
       " '  h': 26,\n",
       " '  h a': 27,\n",
       " '  h a s': 28,\n",
       " '  h a s  ': 29,\n",
       " '  j': 30,\n",
       " '  j u': 31,\n",
       " '  j u m': 32,\n",
       " '  j u m p': 33,\n",
       " '  l': 34,\n",
       " '  l a': 35,\n",
       " '  l a z': 36,\n",
       " '  l a z y': 37,\n",
       " '  l e': 38,\n",
       " '  l e a': 39,\n",
       " '  l e a p': 40,\n",
       " '  o': 41,\n",
       " '  o v': 42,\n",
       " '  o v e': 43,\n",
       " '  o v e r': 44,\n",
       " '  q': 45,\n",
       " '  q u': 46,\n",
       " '  q u i': 47,\n",
       " '  q u i c': 48,\n",
       " '  s': 49,\n",
       " '  s l': 50,\n",
       " '  s l e': 51,\n",
       " '  s l e e': 52,\n",
       " '  s u': 53,\n",
       " '  s u b': 54,\n",
       " '  s u b s': 55,\n",
       " '  t': 56,\n",
       " '  t h': 57,\n",
       " '  t h a': 58,\n",
       " '  t h a t': 59,\n",
       " '  t h e': 60,\n",
       " '  t h e  ': 61,\n",
       " '  t o': 62,\n",
       " '  t o .': 63,\n",
       " '  t w': 64,\n",
       " '  t w i': 65,\n",
       " '  t w i t': 66,\n",
       " '.': 67,\n",
       " 'a': 68,\n",
       " 'a  ': 69,\n",
       " 'a   t': 70,\n",
       " 'a   t w': 71,\n",
       " 'a   t w i': 72,\n",
       " 'a c': 73,\n",
       " 'a c c': 74,\n",
       " 'a c c o': 75,\n",
       " 'a c c o u': 76,\n",
       " 'a p': 77,\n",
       " 'a p s': 78,\n",
       " 'a p s  ': 79,\n",
       " 'a p s   b': 80,\n",
       " 'a s': 81,\n",
       " 'a s  ': 82,\n",
       " 'a s   a': 83,\n",
       " 'a s   a  ': 84,\n",
       " 'a t': 85,\n",
       " 'a t  ': 86,\n",
       " 'a t   t': 87,\n",
       " 'a t   t h': 88,\n",
       " 'a z': 89,\n",
       " 'a z y': 90,\n",
       " 'a z y  ': 91,\n",
       " 'a z y   b': 92,\n",
       " 'a z y   d': 93,\n",
       " 'b': 94,\n",
       " 'b e': 95,\n",
       " 'b e h': 96,\n",
       " 'b e h i': 97,\n",
       " 'b e h i n': 98,\n",
       " 'b e s': 99,\n",
       " 'b e s  ': 100,\n",
       " 'b e s   t': 101,\n",
       " 'b e y': 102,\n",
       " 'b e y o': 103,\n",
       " 'b e y o n': 104,\n",
       " 'b r': 105,\n",
       " 'b r o': 106,\n",
       " 'b r o w': 107,\n",
       " 'b r o w n': 108,\n",
       " 'b s': 109,\n",
       " 'b s c': 110,\n",
       " 'b s c r': 111,\n",
       " 'b s c r i': 112,\n",
       " 'c': 113,\n",
       " 'c c': 114,\n",
       " 'c c o': 115,\n",
       " 'c c o u': 116,\n",
       " 'c c o u n': 117,\n",
       " 'c k': 118,\n",
       " 'c k  ': 119,\n",
       " 'c k   b': 120,\n",
       " 'c k   b r': 121,\n",
       " 'c o': 122,\n",
       " 'c o u': 123,\n",
       " 'c o u n': 124,\n",
       " 'c o u n t': 125,\n",
       " 'c r': 126,\n",
       " 'c r i': 127,\n",
       " 'c r i b': 128,\n",
       " 'c r i b e': 129,\n",
       " 'd': 130,\n",
       " 'd  ': 131,\n",
       " 'd   t': 132,\n",
       " 'd   t h': 133,\n",
       " 'd   t h e': 134,\n",
       " 'd o': 135,\n",
       " 'd o g': 136,\n",
       " 'd o g  ': 137,\n",
       " 'd o g   h': 138,\n",
       " 'd o g .': 139,\n",
       " 'e': 140,\n",
       " 'e  ': 141,\n",
       " 'e   f': 142,\n",
       " 'e   f o': 143,\n",
       " 'e   f o x': 144,\n",
       " 'e   l': 145,\n",
       " 'e   l a': 146,\n",
       " 'e   l a z': 147,\n",
       " 'e   q': 148,\n",
       " 'e   q u': 149,\n",
       " 'e   q u i': 150,\n",
       " 'e   s': 151,\n",
       " 'e   s l': 152,\n",
       " 'e   s l e': 153,\n",
       " 'e a': 154,\n",
       " 'e a p': 155,\n",
       " 'e a p s': 156,\n",
       " 'e a p s  ': 157,\n",
       " 'e e': 158,\n",
       " 'e e p': 159,\n",
       " 'e e p i': 160,\n",
       " 'e e p i n': 161,\n",
       " 'e h': 162,\n",
       " 'e h i': 163,\n",
       " 'e h i n': 164,\n",
       " 'e h i n d': 165,\n",
       " 'e p': 166,\n",
       " 'e p i': 167,\n",
       " 'e p i n': 168,\n",
       " 'e p i n g': 169,\n",
       " 'e r': 170,\n",
       " 'e r  ': 171,\n",
       " 'e r   a': 172,\n",
       " 'e r   a c': 173,\n",
       " 'e r   t': 174,\n",
       " 'e r   t h': 175,\n",
       " 'e s': 176,\n",
       " 'e s  ': 177,\n",
       " 'e s   t': 178,\n",
       " 'e s   t o': 179,\n",
       " 'e y': 180,\n",
       " 'e y o': 181,\n",
       " 'e y o n': 182,\n",
       " 'e y o n d': 183,\n",
       " 'f': 184,\n",
       " 'f o': 185,\n",
       " 'f o x': 186,\n",
       " 'f o x  ': 187,\n",
       " 'f o x   j': 188,\n",
       " 'f o x   l': 189,\n",
       " 'f o x   s': 190,\n",
       " 'g': 191,\n",
       " 'g  ': 192,\n",
       " 'g   d': 193,\n",
       " 'g   d o': 194,\n",
       " 'g   d o g': 195,\n",
       " 'g   h': 196,\n",
       " 'g   h a': 197,\n",
       " 'g   h a s': 198,\n",
       " 'g .': 199,\n",
       " 'h': 200,\n",
       " 'h a': 201,\n",
       " 'h a s': 202,\n",
       " 'h a s  ': 203,\n",
       " 'h a s   a': 204,\n",
       " 'h a t': 205,\n",
       " 'h a t  ': 206,\n",
       " 'h a t   t': 207,\n",
       " 'h e': 208,\n",
       " 'h e  ': 209,\n",
       " 'h e   f': 210,\n",
       " 'h e   f o': 211,\n",
       " 'h e   l': 212,\n",
       " 'h e   l a': 213,\n",
       " 'h e   q': 214,\n",
       " 'h e   q u': 215,\n",
       " 'h e   s': 216,\n",
       " 'h e   s l': 217,\n",
       " 'h i': 218,\n",
       " 'h i n': 219,\n",
       " 'h i n d': 220,\n",
       " 'h i n d  ': 221,\n",
       " 'h i s': 222,\n",
       " 'h i s  ': 223,\n",
       " 'h i s   l': 224,\n",
       " 'i': 225,\n",
       " 'i b': 226,\n",
       " 'i b e': 227,\n",
       " 'i b e s': 228,\n",
       " 'i b e s  ': 229,\n",
       " 'i c': 230,\n",
       " 'i c k': 231,\n",
       " 'i c k  ': 232,\n",
       " 'i c k   b': 233,\n",
       " 'i n': 234,\n",
       " 'i n d': 235,\n",
       " 'i n d  ': 236,\n",
       " 'i n d   t': 237,\n",
       " 'i n g': 238,\n",
       " 'i n g  ': 239,\n",
       " 'i n g   d': 240,\n",
       " 'i s': 241,\n",
       " 'i s  ': 242,\n",
       " 'i s   l': 243,\n",
       " 'i s   l a': 244,\n",
       " 'i t': 245,\n",
       " 'i t t': 246,\n",
       " 'i t t e': 247,\n",
       " 'i t t e r': 248,\n",
       " 'j': 249,\n",
       " 'j u': 250,\n",
       " 'j u m': 251,\n",
       " 'j u m p': 252,\n",
       " 'j u m p s': 253,\n",
       " 'k': 254,\n",
       " 'k  ': 255,\n",
       " 'k   b': 256,\n",
       " 'k   b r': 257,\n",
       " 'k   b r o': 258,\n",
       " 'l': 259,\n",
       " 'l a': 260,\n",
       " 'l a z': 261,\n",
       " 'l a z y': 262,\n",
       " 'l a z y  ': 263,\n",
       " 'l e': 264,\n",
       " 'l e a': 265,\n",
       " 'l e a p': 266,\n",
       " 'l e a p s': 267,\n",
       " 'l e e': 268,\n",
       " 'l e e p': 269,\n",
       " 'l e e p i': 270,\n",
       " 'm': 271,\n",
       " 'm p': 272,\n",
       " 'm p s': 273,\n",
       " 'm p s  ': 274,\n",
       " 'm p s   b': 275,\n",
       " 'm p s   o': 276,\n",
       " 'n': 277,\n",
       " 'n  ': 278,\n",
       " 'n   f': 279,\n",
       " 'n   f o': 280,\n",
       " 'n   f o x': 281,\n",
       " 'n d': 282,\n",
       " 'n d  ': 283,\n",
       " 'n d   t': 284,\n",
       " 'n d   t h': 285,\n",
       " 'n g': 286,\n",
       " 'n g  ': 287,\n",
       " 'n g   d': 288,\n",
       " 'n g   d o': 289,\n",
       " 'n t': 290,\n",
       " 'n t  ': 291,\n",
       " 'n t   t': 292,\n",
       " 'n t   t h': 293,\n",
       " 'o': 294,\n",
       " 'o .': 295,\n",
       " 'o g': 296,\n",
       " 'o g  ': 297,\n",
       " 'o g   h': 298,\n",
       " 'o g   h a': 299,\n",
       " 'o g .': 300,\n",
       " 'o n': 301,\n",
       " 'o n d': 302,\n",
       " 'o n d  ': 303,\n",
       " 'o n d   t': 304,\n",
       " 'o u': 305,\n",
       " 'o u n': 306,\n",
       " 'o u n t': 307,\n",
       " 'o u n t  ': 308,\n",
       " 'o v': 309,\n",
       " 'o v e': 310,\n",
       " 'o v e r': 311,\n",
       " 'o v e r  ': 312,\n",
       " 'o w': 313,\n",
       " 'o w n': 314,\n",
       " 'o w n  ': 315,\n",
       " 'o w n   f': 316,\n",
       " 'o x': 317,\n",
       " 'o x  ': 318,\n",
       " 'o x   j': 319,\n",
       " 'o x   j u': 320,\n",
       " 'o x   l': 321,\n",
       " 'o x   l e': 322,\n",
       " 'o x   s': 323,\n",
       " 'o x   s u': 324,\n",
       " 'p': 325,\n",
       " 'p i': 326,\n",
       " 'p i n': 327,\n",
       " 'p i n g': 328,\n",
       " 'p i n g  ': 329,\n",
       " 'p s': 330,\n",
       " 'p s  ': 331,\n",
       " 'p s   b': 332,\n",
       " 'p s   b e': 333,\n",
       " 'p s   o': 334,\n",
       " 'p s   o v': 335,\n",
       " 'q': 336,\n",
       " 'q u': 337,\n",
       " 'q u i': 338,\n",
       " 'q u i c': 339,\n",
       " 'q u i c k': 340,\n",
       " 'r': 341,\n",
       " 'r  ': 342,\n",
       " 'r   a': 343,\n",
       " 'r   a c': 344,\n",
       " 'r   a c c': 345,\n",
       " 'r   t': 346,\n",
       " 'r   t h': 347,\n",
       " 'r   t h e': 348,\n",
       " 'r i': 349,\n",
       " 'r i b': 350,\n",
       " 'r i b e': 351,\n",
       " 'r i b e s': 352,\n",
       " 'r o': 353,\n",
       " 'r o w': 354,\n",
       " 'r o w n': 355,\n",
       " 'r o w n  ': 356,\n",
       " 's': 357,\n",
       " 's  ': 358,\n",
       " 's   a': 359,\n",
       " 's   a  ': 360,\n",
       " 's   a   t': 361,\n",
       " 's   b': 362,\n",
       " 's   b e': 363,\n",
       " 's   b e h': 364,\n",
       " 's   b e y': 365,\n",
       " 's   l': 366,\n",
       " 's   l a': 367,\n",
       " 's   l a z': 368,\n",
       " 's   o': 369,\n",
       " 's   o v': 370,\n",
       " 's   o v e': 371,\n",
       " 's   t': 372,\n",
       " 's   t o': 373,\n",
       " 's   t o .': 374,\n",
       " 's c': 375,\n",
       " 's c r': 376,\n",
       " 's c r i': 377,\n",
       " 's c r i b': 378,\n",
       " 's l': 379,\n",
       " 's l e': 380,\n",
       " 's l e e': 381,\n",
       " 's l e e p': 382,\n",
       " 's u': 383,\n",
       " 's u b': 384,\n",
       " 's u b s': 385,\n",
       " 's u b s c': 386,\n",
       " 't': 387,\n",
       " 't  ': 388,\n",
       " 't   t': 389,\n",
       " 't   t h': 390,\n",
       " 't   t h a': 391,\n",
       " 't   t h e': 392,\n",
       " 't e': 393,\n",
       " 't e r': 394,\n",
       " 't e r  ': 395,\n",
       " 't e r   a': 396,\n",
       " 't h': 397,\n",
       " 't h a': 398,\n",
       " 't h a t': 399,\n",
       " 't h a t  ': 400,\n",
       " 't h e': 401,\n",
       " 't h e  ': 402,\n",
       " 't h e   f': 403,\n",
       " 't h e   l': 404,\n",
       " 't h e   q': 405,\n",
       " 't h e   s': 406,\n",
       " 't h i': 407,\n",
       " 't h i s': 408,\n",
       " 't h i s  ': 409,\n",
       " 't o': 410,\n",
       " 't o .': 411,\n",
       " 't t': 412,\n",
       " 't t e': 413,\n",
       " 't t e r': 414,\n",
       " 't t e r  ': 415,\n",
       " 't w': 416,\n",
       " 't w i': 417,\n",
       " 't w i t': 418,\n",
       " 't w i t t': 419,\n",
       " 'u': 420,\n",
       " 'u b': 421,\n",
       " 'u b s': 422,\n",
       " 'u b s c': 423,\n",
       " 'u b s c r': 424,\n",
       " 'u i': 425,\n",
       " 'u i c': 426,\n",
       " 'u i c k': 427,\n",
       " 'u i c k  ': 428,\n",
       " 'u m': 429,\n",
       " 'u m p': 430,\n",
       " 'u m p s': 431,\n",
       " 'u m p s  ': 432,\n",
       " 'u n': 433,\n",
       " 'u n t': 434,\n",
       " 'u n t  ': 435,\n",
       " 'u n t   t': 436,\n",
       " 'v': 437,\n",
       " 'v e': 438,\n",
       " 'v e r': 439,\n",
       " 'v e r  ': 440,\n",
       " 'v e r   t': 441,\n",
       " 'w': 442,\n",
       " 'w i': 443,\n",
       " 'w i t': 444,\n",
       " 'w i t t': 445,\n",
       " 'w i t t e': 446,\n",
       " 'w n': 447,\n",
       " 'w n  ': 448,\n",
       " 'w n   f': 449,\n",
       " 'w n   f o': 450,\n",
       " 'x': 451,\n",
       " 'x  ': 452,\n",
       " 'x   j': 453,\n",
       " 'x   j u': 454,\n",
       " 'x   j u m': 455,\n",
       " 'x   l': 456,\n",
       " 'x   l e': 457,\n",
       " 'x   l e a': 458,\n",
       " 'x   s': 459,\n",
       " 'x   s u': 460,\n",
       " 'x   s u b': 461,\n",
       " 'y': 462,\n",
       " 'y  ': 463,\n",
       " 'y   b': 464,\n",
       " 'y   b r': 465,\n",
       " 'y   b r o': 466,\n",
       " 'y   d': 467,\n",
       " 'y   d o': 468,\n",
       " 'y   d o g': 469,\n",
       " 'y o': 470,\n",
       " 'y o n': 471,\n",
       " 'y o n d': 472,\n",
       " 'y o n d  ': 473,\n",
       " 'z': 474,\n",
       " 'z y': 475,\n",
       " 'z y  ': 476,\n",
       " 'z y   b': 477,\n",
       " 'z y   b r': 478,\n",
       " 'z y   d': 479,\n",
       " 'z y   d o': 480}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "character_vectorizer.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Change character n-grams dictionary.\n",
    "* Character n-grams come after the 49 word n-grams, shift the index accordingly\n",
    "* Space symbol has index 0, but new index *after* all word n-grams is: `0 + 49 = 49`\n",
    "* Bigram \" a\" with index 1, but its new index is: 1 + 49 = 50, etc. \n",
    "\n",
    "### Add 49 to all of indices of the Character Dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_ngrams_count = len(word_vectorizer.vocabulary_)\n",
    "\n",
    "new_character_vocabulary = {}\n",
    "\n",
    "for character_ngram, index in character_vectorizer.vocabulary_.items():\n",
    "    new_character_vocabulary[character_ngram] = index + word_ngrams_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{' ': 49,\n",
       " '  a': 50,\n",
       " '  a  ': 51,\n",
       " '  a   t': 52,\n",
       " '  a   t w': 53,\n",
       " '  a c': 54,\n",
       " '  a c c': 55,\n",
       " '  a c c o': 56,\n",
       " '  b': 57,\n",
       " '  b e': 58,\n",
       " '  b e h': 59,\n",
       " '  b e h i': 60,\n",
       " '  b e y': 61,\n",
       " '  b e y o': 62,\n",
       " '  b r': 63,\n",
       " '  b r o': 64,\n",
       " '  b r o w': 65,\n",
       " '  d': 66,\n",
       " '  d o': 67,\n",
       " '  d o g': 68,\n",
       " '  d o g  ': 69,\n",
       " '  d o g .': 70,\n",
       " '  f': 71,\n",
       " '  f o': 72,\n",
       " '  f o x': 73,\n",
       " '  f o x  ': 74,\n",
       " '  h': 75,\n",
       " '  h a': 76,\n",
       " '  h a s': 77,\n",
       " '  h a s  ': 78,\n",
       " '  j': 79,\n",
       " '  j u': 80,\n",
       " '  j u m': 81,\n",
       " '  j u m p': 82,\n",
       " '  l': 83,\n",
       " '  l a': 84,\n",
       " '  l a z': 85,\n",
       " '  l a z y': 86,\n",
       " '  l e': 87,\n",
       " '  l e a': 88,\n",
       " '  l e a p': 89,\n",
       " '  o': 90,\n",
       " '  o v': 91,\n",
       " '  o v e': 92,\n",
       " '  o v e r': 93,\n",
       " '  q': 94,\n",
       " '  q u': 95,\n",
       " '  q u i': 96,\n",
       " '  q u i c': 97,\n",
       " '  s': 98,\n",
       " '  s l': 99,\n",
       " '  s l e': 100,\n",
       " '  s l e e': 101,\n",
       " '  s u': 102,\n",
       " '  s u b': 103,\n",
       " '  s u b s': 104,\n",
       " '  t': 105,\n",
       " '  t h': 106,\n",
       " '  t h a': 107,\n",
       " '  t h a t': 108,\n",
       " '  t h e': 109,\n",
       " '  t h e  ': 110,\n",
       " '  t o': 111,\n",
       " '  t o .': 112,\n",
       " '  t w': 113,\n",
       " '  t w i': 114,\n",
       " '  t w i t': 115,\n",
       " '.': 116,\n",
       " 'a': 117,\n",
       " 'a  ': 118,\n",
       " 'a   t': 119,\n",
       " 'a   t w': 120,\n",
       " 'a   t w i': 121,\n",
       " 'a c': 122,\n",
       " 'a c c': 123,\n",
       " 'a c c o': 124,\n",
       " 'a c c o u': 125,\n",
       " 'a p': 126,\n",
       " 'a p s': 127,\n",
       " 'a p s  ': 128,\n",
       " 'a p s   b': 129,\n",
       " 'a s': 130,\n",
       " 'a s  ': 131,\n",
       " 'a s   a': 132,\n",
       " 'a s   a  ': 133,\n",
       " 'a t': 134,\n",
       " 'a t  ': 135,\n",
       " 'a t   t': 136,\n",
       " 'a t   t h': 137,\n",
       " 'a z': 138,\n",
       " 'a z y': 139,\n",
       " 'a z y  ': 140,\n",
       " 'a z y   b': 141,\n",
       " 'a z y   d': 142,\n",
       " 'b': 143,\n",
       " 'b e': 144,\n",
       " 'b e h': 145,\n",
       " 'b e h i': 146,\n",
       " 'b e h i n': 147,\n",
       " 'b e s': 148,\n",
       " 'b e s  ': 149,\n",
       " 'b e s   t': 150,\n",
       " 'b e y': 151,\n",
       " 'b e y o': 152,\n",
       " 'b e y o n': 153,\n",
       " 'b r': 154,\n",
       " 'b r o': 155,\n",
       " 'b r o w': 156,\n",
       " 'b r o w n': 157,\n",
       " 'b s': 158,\n",
       " 'b s c': 159,\n",
       " 'b s c r': 160,\n",
       " 'b s c r i': 161,\n",
       " 'c': 162,\n",
       " 'c c': 163,\n",
       " 'c c o': 164,\n",
       " 'c c o u': 165,\n",
       " 'c c o u n': 166,\n",
       " 'c k': 167,\n",
       " 'c k  ': 168,\n",
       " 'c k   b': 169,\n",
       " 'c k   b r': 170,\n",
       " 'c o': 171,\n",
       " 'c o u': 172,\n",
       " 'c o u n': 173,\n",
       " 'c o u n t': 174,\n",
       " 'c r': 175,\n",
       " 'c r i': 176,\n",
       " 'c r i b': 177,\n",
       " 'c r i b e': 178,\n",
       " 'd': 179,\n",
       " 'd  ': 180,\n",
       " 'd   t': 181,\n",
       " 'd   t h': 182,\n",
       " 'd   t h e': 183,\n",
       " 'd o': 184,\n",
       " 'd o g': 185,\n",
       " 'd o g  ': 186,\n",
       " 'd o g   h': 187,\n",
       " 'd o g .': 188,\n",
       " 'e': 189,\n",
       " 'e  ': 190,\n",
       " 'e   f': 191,\n",
       " 'e   f o': 192,\n",
       " 'e   f o x': 193,\n",
       " 'e   l': 194,\n",
       " 'e   l a': 195,\n",
       " 'e   l a z': 196,\n",
       " 'e   q': 197,\n",
       " 'e   q u': 198,\n",
       " 'e   q u i': 199,\n",
       " 'e   s': 200,\n",
       " 'e   s l': 201,\n",
       " 'e   s l e': 202,\n",
       " 'e a': 203,\n",
       " 'e a p': 204,\n",
       " 'e a p s': 205,\n",
       " 'e a p s  ': 206,\n",
       " 'e e': 207,\n",
       " 'e e p': 208,\n",
       " 'e e p i': 209,\n",
       " 'e e p i n': 210,\n",
       " 'e h': 211,\n",
       " 'e h i': 212,\n",
       " 'e h i n': 213,\n",
       " 'e h i n d': 214,\n",
       " 'e p': 215,\n",
       " 'e p i': 216,\n",
       " 'e p i n': 217,\n",
       " 'e p i n g': 218,\n",
       " 'e r': 219,\n",
       " 'e r  ': 220,\n",
       " 'e r   a': 221,\n",
       " 'e r   a c': 222,\n",
       " 'e r   t': 223,\n",
       " 'e r   t h': 224,\n",
       " 'e s': 225,\n",
       " 'e s  ': 226,\n",
       " 'e s   t': 227,\n",
       " 'e s   t o': 228,\n",
       " 'e y': 229,\n",
       " 'e y o': 230,\n",
       " 'e y o n': 231,\n",
       " 'e y o n d': 232,\n",
       " 'f': 233,\n",
       " 'f o': 234,\n",
       " 'f o x': 235,\n",
       " 'f o x  ': 236,\n",
       " 'f o x   j': 237,\n",
       " 'f o x   l': 238,\n",
       " 'f o x   s': 239,\n",
       " 'g': 240,\n",
       " 'g  ': 241,\n",
       " 'g   d': 242,\n",
       " 'g   d o': 243,\n",
       " 'g   d o g': 244,\n",
       " 'g   h': 245,\n",
       " 'g   h a': 246,\n",
       " 'g   h a s': 247,\n",
       " 'g .': 248,\n",
       " 'h': 249,\n",
       " 'h a': 250,\n",
       " 'h a s': 251,\n",
       " 'h a s  ': 252,\n",
       " 'h a s   a': 253,\n",
       " 'h a t': 254,\n",
       " 'h a t  ': 255,\n",
       " 'h a t   t': 256,\n",
       " 'h e': 257,\n",
       " 'h e  ': 258,\n",
       " 'h e   f': 259,\n",
       " 'h e   f o': 260,\n",
       " 'h e   l': 261,\n",
       " 'h e   l a': 262,\n",
       " 'h e   q': 263,\n",
       " 'h e   q u': 264,\n",
       " 'h e   s': 265,\n",
       " 'h e   s l': 266,\n",
       " 'h i': 267,\n",
       " 'h i n': 268,\n",
       " 'h i n d': 269,\n",
       " 'h i n d  ': 270,\n",
       " 'h i s': 271,\n",
       " 'h i s  ': 272,\n",
       " 'h i s   l': 273,\n",
       " 'i': 274,\n",
       " 'i b': 275,\n",
       " 'i b e': 276,\n",
       " 'i b e s': 277,\n",
       " 'i b e s  ': 278,\n",
       " 'i c': 279,\n",
       " 'i c k': 280,\n",
       " 'i c k  ': 281,\n",
       " 'i c k   b': 282,\n",
       " 'i n': 283,\n",
       " 'i n d': 284,\n",
       " 'i n d  ': 285,\n",
       " 'i n d   t': 286,\n",
       " 'i n g': 287,\n",
       " 'i n g  ': 288,\n",
       " 'i n g   d': 289,\n",
       " 'i s': 290,\n",
       " 'i s  ': 291,\n",
       " 'i s   l': 292,\n",
       " 'i s   l a': 293,\n",
       " 'i t': 294,\n",
       " 'i t t': 295,\n",
       " 'i t t e': 296,\n",
       " 'i t t e r': 297,\n",
       " 'j': 298,\n",
       " 'j u': 299,\n",
       " 'j u m': 300,\n",
       " 'j u m p': 301,\n",
       " 'j u m p s': 302,\n",
       " 'k': 303,\n",
       " 'k  ': 304,\n",
       " 'k   b': 305,\n",
       " 'k   b r': 306,\n",
       " 'k   b r o': 307,\n",
       " 'l': 308,\n",
       " 'l a': 309,\n",
       " 'l a z': 310,\n",
       " 'l a z y': 311,\n",
       " 'l a z y  ': 312,\n",
       " 'l e': 313,\n",
       " 'l e a': 314,\n",
       " 'l e a p': 315,\n",
       " 'l e a p s': 316,\n",
       " 'l e e': 317,\n",
       " 'l e e p': 318,\n",
       " 'l e e p i': 319,\n",
       " 'm': 320,\n",
       " 'm p': 321,\n",
       " 'm p s': 322,\n",
       " 'm p s  ': 323,\n",
       " 'm p s   b': 324,\n",
       " 'm p s   o': 325,\n",
       " 'n': 326,\n",
       " 'n  ': 327,\n",
       " 'n   f': 328,\n",
       " 'n   f o': 329,\n",
       " 'n   f o x': 330,\n",
       " 'n d': 331,\n",
       " 'n d  ': 332,\n",
       " 'n d   t': 333,\n",
       " 'n d   t h': 334,\n",
       " 'n g': 335,\n",
       " 'n g  ': 336,\n",
       " 'n g   d': 337,\n",
       " 'n g   d o': 338,\n",
       " 'n t': 339,\n",
       " 'n t  ': 340,\n",
       " 'n t   t': 341,\n",
       " 'n t   t h': 342,\n",
       " 'o': 343,\n",
       " 'o .': 344,\n",
       " 'o g': 345,\n",
       " 'o g  ': 346,\n",
       " 'o g   h': 347,\n",
       " 'o g   h a': 348,\n",
       " 'o g .': 349,\n",
       " 'o n': 350,\n",
       " 'o n d': 351,\n",
       " 'o n d  ': 352,\n",
       " 'o n d   t': 353,\n",
       " 'o u': 354,\n",
       " 'o u n': 355,\n",
       " 'o u n t': 356,\n",
       " 'o u n t  ': 357,\n",
       " 'o v': 358,\n",
       " 'o v e': 359,\n",
       " 'o v e r': 360,\n",
       " 'o v e r  ': 361,\n",
       " 'o w': 362,\n",
       " 'o w n': 363,\n",
       " 'o w n  ': 364,\n",
       " 'o w n   f': 365,\n",
       " 'o x': 366,\n",
       " 'o x  ': 367,\n",
       " 'o x   j': 368,\n",
       " 'o x   j u': 369,\n",
       " 'o x   l': 370,\n",
       " 'o x   l e': 371,\n",
       " 'o x   s': 372,\n",
       " 'o x   s u': 373,\n",
       " 'p': 374,\n",
       " 'p i': 375,\n",
       " 'p i n': 376,\n",
       " 'p i n g': 377,\n",
       " 'p i n g  ': 378,\n",
       " 'p s': 379,\n",
       " 'p s  ': 380,\n",
       " 'p s   b': 381,\n",
       " 'p s   b e': 382,\n",
       " 'p s   o': 383,\n",
       " 'p s   o v': 384,\n",
       " 'q': 385,\n",
       " 'q u': 386,\n",
       " 'q u i': 387,\n",
       " 'q u i c': 388,\n",
       " 'q u i c k': 389,\n",
       " 'r': 390,\n",
       " 'r  ': 391,\n",
       " 'r   a': 392,\n",
       " 'r   a c': 393,\n",
       " 'r   a c c': 394,\n",
       " 'r   t': 395,\n",
       " 'r   t h': 396,\n",
       " 'r   t h e': 397,\n",
       " 'r i': 398,\n",
       " 'r i b': 399,\n",
       " 'r i b e': 400,\n",
       " 'r i b e s': 401,\n",
       " 'r o': 402,\n",
       " 'r o w': 403,\n",
       " 'r o w n': 404,\n",
       " 'r o w n  ': 405,\n",
       " 's': 406,\n",
       " 's  ': 407,\n",
       " 's   a': 408,\n",
       " 's   a  ': 409,\n",
       " 's   a   t': 410,\n",
       " 's   b': 411,\n",
       " 's   b e': 412,\n",
       " 's   b e h': 413,\n",
       " 's   b e y': 414,\n",
       " 's   l': 415,\n",
       " 's   l a': 416,\n",
       " 's   l a z': 417,\n",
       " 's   o': 418,\n",
       " 's   o v': 419,\n",
       " 's   o v e': 420,\n",
       " 's   t': 421,\n",
       " 's   t o': 422,\n",
       " 's   t o .': 423,\n",
       " 's c': 424,\n",
       " 's c r': 425,\n",
       " 's c r i': 426,\n",
       " 's c r i b': 427,\n",
       " 's l': 428,\n",
       " 's l e': 429,\n",
       " 's l e e': 430,\n",
       " 's l e e p': 431,\n",
       " 's u': 432,\n",
       " 's u b': 433,\n",
       " 's u b s': 434,\n",
       " 's u b s c': 435,\n",
       " 't': 436,\n",
       " 't  ': 437,\n",
       " 't   t': 438,\n",
       " 't   t h': 439,\n",
       " 't   t h a': 440,\n",
       " 't   t h e': 441,\n",
       " 't e': 442,\n",
       " 't e r': 443,\n",
       " 't e r  ': 444,\n",
       " 't e r   a': 445,\n",
       " 't h': 446,\n",
       " 't h a': 447,\n",
       " 't h a t': 448,\n",
       " 't h a t  ': 449,\n",
       " 't h e': 450,\n",
       " 't h e  ': 451,\n",
       " 't h e   f': 452,\n",
       " 't h e   l': 453,\n",
       " 't h e   q': 454,\n",
       " 't h e   s': 455,\n",
       " 't h i': 456,\n",
       " 't h i s': 457,\n",
       " 't h i s  ': 458,\n",
       " 't o': 459,\n",
       " 't o .': 460,\n",
       " 't t': 461,\n",
       " 't t e': 462,\n",
       " 't t e r': 463,\n",
       " 't t e r  ': 464,\n",
       " 't w': 465,\n",
       " 't w i': 466,\n",
       " 't w i t': 467,\n",
       " 't w i t t': 468,\n",
       " 'u': 469,\n",
       " 'u b': 470,\n",
       " 'u b s': 471,\n",
       " 'u b s c': 472,\n",
       " 'u b s c r': 473,\n",
       " 'u i': 474,\n",
       " 'u i c': 475,\n",
       " 'u i c k': 476,\n",
       " 'u i c k  ': 477,\n",
       " 'u m': 478,\n",
       " 'u m p': 479,\n",
       " 'u m p s': 480,\n",
       " 'u m p s  ': 481,\n",
       " 'u n': 482,\n",
       " 'u n t': 483,\n",
       " 'u n t  ': 484,\n",
       " 'u n t   t': 485,\n",
       " 'v': 486,\n",
       " 'v e': 487,\n",
       " 'v e r': 488,\n",
       " 'v e r  ': 489,\n",
       " 'v e r   t': 490,\n",
       " 'w': 491,\n",
       " 'w i': 492,\n",
       " 'w i t': 493,\n",
       " 'w i t t': 494,\n",
       " 'w i t t e': 495,\n",
       " 'w n': 496,\n",
       " 'w n  ': 497,\n",
       " 'w n   f': 498,\n",
       " 'w n   f o': 499,\n",
       " 'x': 500,\n",
       " 'x  ': 501,\n",
       " 'x   j': 502,\n",
       " 'x   j u': 503,\n",
       " 'x   j u m': 504,\n",
       " 'x   l': 505,\n",
       " 'x   l e': 506,\n",
       " 'x   l e a': 507,\n",
       " 'x   s': 508,\n",
       " 'x   s u': 509,\n",
       " 'x   s u b': 510,\n",
       " 'y': 511,\n",
       " 'y  ': 512,\n",
       " 'y   b': 513,\n",
       " 'y   b r': 514,\n",
       " 'y   b r o': 515,\n",
       " 'y   d': 516,\n",
       " 'y   d o': 517,\n",
       " 'y   d o g': 518,\n",
       " 'y o': 519,\n",
       " 'y o n': 520,\n",
       " 'y o n d': 521,\n",
       " 'y o n d  ': 522,\n",
       " 'z': 523,\n",
       " 'z y': 524,\n",
       " 'z y  ': 525,\n",
       " 'z y   b': 526,\n",
       " 'z y   b r': 527,\n",
       " 'z y   d': 528,\n",
       " 'z y   d o': 529}"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_character_vocabulary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with final matrix\n",
    "* Use ``word_vectorizer.vocabulary_`` for Word n-grams to find which words correspond to which index\n",
    "* Use ``new_character_vocabulary`` for the Character n-grams. "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
